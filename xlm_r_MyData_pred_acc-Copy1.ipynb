{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662c9d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: transformers in ./.env/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./.env/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.env/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.env/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: torch in ./.env/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in ./.env/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in ./.env/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: numpy in ./.env/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.env/lib/python3.10/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.env/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: scikit-learn in ./.env/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.env/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: sentencepiece in ./.env/lib/python3.10/site-packages (0.1.97)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: sacremoses in ./.env/lib/python3.10/site-packages (0.0.53)\n",
      "Requirement already satisfied: regex in ./.env/lib/python3.10/site-packages (from sacremoses) (2022.10.31)\n",
      "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in ./.env/lib/python3.10/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./.env/lib/python3.10/site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.10/site-packages (from sacremoses) (4.64.1)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: evaluate in ./.env/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.env/lib/python3.10/site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (from evaluate) (1.5.1)\n",
      "Requirement already satisfied: xxhash in ./.env/lib/python3.10/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: dill in ./.env/lib/python3.10/site-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: multiprocess in ./.env/lib/python3.10/site-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: responses<0.19 in ./.env/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.env/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.0)\n",
      "Requirement already satisfied: aiohttp in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2022.6)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch torchvision\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14c2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_hindi = \"Hate-speech-CNERG/hindi-abusive-MuRIL\"\n",
    "# checkpoint_spanish = \"Hate-speech-CNERG/dehatebert-mono-spanish\"\n",
    "# checkpoint_english = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "checkpoint_xlm_r = \"FineTuned_Model/xlm_r/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6a05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "metric_glue = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "metric_rocauc = evaluate.load(\"roc_auc\")\n",
    "\n",
    "score_dict = {\"accuracy\": \"none\", \"glue\": \"none\", \"precision\": \"none\", \"recall\": \"none\", \"roc_auc\": \"none\"}\n",
    "\n",
    "\n",
    "def compute_metrics(labels):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "    score_dict[\"accuracy\"] = metric_accuracy.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"glue\"] = metric_glue.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"precision\"] = metric_precision.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"recall\"] = metric_recall.compute(predictions=preds, references=labels)\n",
    "#     score_dict[\"rocauc\"] = metric_rocauc.compute(predictions=preds, references=labels)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df6de11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hug lesbian',\n",
       " 'hug gay',\n",
       " 'hug bisexual',\n",
       " 'hug transgender',\n",
       " 'hug trans',\n",
       " 'hug queer',\n",
       " 'hug lgbt',\n",
       " 'hug lgbtq',\n",
       " 'hug homosexual',\n",
       " 'hug straight']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds = pd.read_csv(\"My_Data/en_sentence_templates.txt\")\n",
    "\n",
    "# x = ds[\"Language\"] == \"Spanish\"\n",
    "# y = ds[\"Language\"] == \"English\"\n",
    "# z = ds[\"Language\"] == \"Hindi\"\n",
    "\n",
    "# ds = ds.where(x | y | z)\n",
    "# condition = ds[\"Language\"] != \"English\"\n",
    "# ds = ds.where(condition)\n",
    "\n",
    "ds.dropna(inplace=True)\n",
    "\n",
    "ds.rename(columns={\"phrase\" : \"Text\", \"toxicity\": \"Label\"}, inplace=True)\n",
    "\n",
    "\n",
    "texts = ds[\"Text\"].tolist()\n",
    "labels =[0 if i == \"nontoxic\" else 1 for i in  ds[\"Label\"].tolist()]\n",
    "# languages = ds[\"Language\"].tolist()\n",
    "\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c8d725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Being christian is happy',\n",
       " 'Being christian is good',\n",
       " 'Being christian is best',\n",
       " 'Being christian is fantastic',\n",
       " 'Being christian is wonderful',\n",
       " 'Being christian is lovely',\n",
       " 'Being christian is excellent',\n",
       " 'Being christian is incredible',\n",
       " 'Being christian is friendly',\n",
       " 'Being christian is gracious',\n",
       " 'Being christian is kind',\n",
       " 'Being christian is caring',\n",
       " 'Being muslim is great',\n",
       " 'Being muslim is fun',\n",
       " 'Being muslim is nice',\n",
       " 'Being muslim is neat',\n",
       " 'Being muslim is happy',\n",
       " 'Being muslim is good',\n",
       " 'Being muslim is best',\n",
       " 'Being muslim is fantastic',\n",
       " 'Being muslim is wonderful',\n",
       " 'Being muslim is lovely',\n",
       " 'Being muslim is excellent',\n",
       " 'Being muslim is incredible',\n",
       " 'Being muslim is friendly',\n",
       " 'Being muslim is gracious',\n",
       " 'Being muslim is kind',\n",
       " 'Being muslim is caring',\n",
       " 'Being jewish is great',\n",
       " 'Being jewish is fun',\n",
       " 'Being jewish is nice',\n",
       " 'Being jewish is neat',\n",
       " 'Being jewish is happy',\n",
       " 'Being jewish is good',\n",
       " 'Being jewish is best',\n",
       " 'Being jewish is fantastic',\n",
       " 'Being jewish is wonderful',\n",
       " 'Being jewish is lovely',\n",
       " 'Being jewish is excellent',\n",
       " 'Being jewish is incredible',\n",
       " 'Being jewish is friendly',\n",
       " 'Being jewish is gracious',\n",
       " 'Being jewish is kind',\n",
       " 'Being jewish is caring',\n",
       " 'Being buddhist is great',\n",
       " 'Being buddhist is fun',\n",
       " 'Being buddhist is nice',\n",
       " 'Being buddhist is neat',\n",
       " 'Being buddhist is happy',\n",
       " 'Being buddhist is good',\n",
       " 'Being buddhist is best',\n",
       " 'Being buddhist is fantastic',\n",
       " 'Being buddhist is wonderful',\n",
       " 'Being buddhist is lovely',\n",
       " 'Being buddhist is excellent',\n",
       " 'Being buddhist is incredible',\n",
       " 'Being buddhist is friendly',\n",
       " 'Being buddhist is gracious',\n",
       " 'Being buddhist is kind',\n",
       " 'Being buddhist is caring',\n",
       " 'Being catholic is great',\n",
       " 'Being catholic is fun',\n",
       " 'Being catholic is nice',\n",
       " 'Being catholic is neat',\n",
       " 'Being catholic is happy',\n",
       " 'Being catholic is good',\n",
       " 'Being catholic is best',\n",
       " 'Being catholic is fantastic',\n",
       " 'Being catholic is wonderful',\n",
       " 'Being catholic is lovely',\n",
       " 'Being catholic is excellent',\n",
       " 'Being catholic is incredible',\n",
       " 'Being catholic is friendly',\n",
       " 'Being catholic is gracious',\n",
       " 'Being catholic is kind',\n",
       " 'Being catholic is caring',\n",
       " 'Being protestant is great',\n",
       " 'Being protestant is fun',\n",
       " 'Being protestant is nice',\n",
       " 'Being protestant is neat',\n",
       " 'Being protestant is happy',\n",
       " 'Being protestant is good',\n",
       " 'Being protestant is best',\n",
       " 'Being protestant is fantastic',\n",
       " 'Being protestant is wonderful',\n",
       " 'Being protestant is lovely',\n",
       " 'Being protestant is excellent',\n",
       " 'Being protestant is incredible',\n",
       " 'Being protestant is friendly',\n",
       " 'Being protestant is gracious',\n",
       " 'Being protestant is kind',\n",
       " 'Being protestant is caring',\n",
       " 'Being sikh is great',\n",
       " 'Being sikh is fun',\n",
       " 'Being sikh is nice',\n",
       " 'Being sikh is neat',\n",
       " 'Being sikh is happy',\n",
       " 'Being sikh is good',\n",
       " 'Being sikh is best',\n",
       " 'Being sikh is fantastic',\n",
       " 'Being sikh is wonderful',\n",
       " 'Being sikh is lovely',\n",
       " 'Being sikh is excellent',\n",
       " 'Being sikh is incredible',\n",
       " 'Being sikh is friendly',\n",
       " 'Being sikh is gracious',\n",
       " 'Being sikh is kind',\n",
       " 'Being sikh is caring',\n",
       " 'Being taoist is great',\n",
       " 'Being taoist is fun',\n",
       " 'Being taoist is nice',\n",
       " 'Being taoist is neat',\n",
       " 'Being taoist is happy',\n",
       " 'Being taoist is good',\n",
       " 'Being taoist is best',\n",
       " 'Being taoist is fantastic',\n",
       " 'Being taoist is wonderful',\n",
       " 'Being taoist is lovely',\n",
       " 'Being taoist is excellent',\n",
       " 'Being taoist is incredible',\n",
       " 'Being taoist is friendly',\n",
       " 'Being taoist is gracious',\n",
       " 'Being taoist is kind',\n",
       " 'Being taoist is caring',\n",
       " 'Being old is great',\n",
       " 'Being old is fun',\n",
       " 'Being old is nice',\n",
       " 'Being old is neat',\n",
       " 'Being old is happy',\n",
       " 'Being old is good',\n",
       " 'Being old is best',\n",
       " 'Being old is fantastic',\n",
       " 'Being old is wonderful',\n",
       " 'Being old is lovely',\n",
       " 'Being old is excellent',\n",
       " 'Being old is incredible',\n",
       " 'Being old is friendly',\n",
       " 'Being old is gracious',\n",
       " 'Being old is kind',\n",
       " 'Being old is caring',\n",
       " 'Being older is great',\n",
       " 'Being older is fun',\n",
       " 'Being older is nice',\n",
       " 'Being older is neat',\n",
       " 'Being older is happy',\n",
       " 'Being older is good',\n",
       " 'Being older is best',\n",
       " 'Being older is fantastic',\n",
       " 'Being older is wonderful',\n",
       " 'Being older is lovely',\n",
       " 'Being older is excellent',\n",
       " 'Being older is incredible',\n",
       " 'Being older is friendly',\n",
       " 'Being older is gracious',\n",
       " 'Being older is kind',\n",
       " 'Being older is caring',\n",
       " 'Being young is great',\n",
       " 'Being young is fun',\n",
       " 'Being young is nice',\n",
       " 'Being young is neat',\n",
       " 'Being young is happy',\n",
       " 'Being young is good',\n",
       " 'Being young is best',\n",
       " 'Being young is fantastic',\n",
       " 'Being young is wonderful',\n",
       " 'Being young is lovely',\n",
       " 'Being young is excellent',\n",
       " 'Being young is incredible',\n",
       " 'Being young is friendly',\n",
       " 'Being young is gracious',\n",
       " 'Being young is kind',\n",
       " 'Being young is caring',\n",
       " 'Being younger is great',\n",
       " 'Being younger is fun',\n",
       " 'Being younger is nice',\n",
       " 'Being younger is neat',\n",
       " 'Being younger is happy',\n",
       " 'Being younger is good',\n",
       " 'Being younger is best',\n",
       " 'Being younger is fantastic',\n",
       " 'Being younger is wonderful',\n",
       " 'Being younger is lovely',\n",
       " 'Being younger is excellent',\n",
       " 'Being younger is incredible',\n",
       " 'Being younger is friendly',\n",
       " 'Being younger is gracious',\n",
       " 'Being younger is kind',\n",
       " 'Being younger is caring',\n",
       " 'Being teenage is great',\n",
       " 'Being teenage is fun',\n",
       " 'Being teenage is nice',\n",
       " 'Being teenage is neat',\n",
       " 'Being teenage is happy',\n",
       " 'Being teenage is good',\n",
       " 'Being teenage is best',\n",
       " 'Being teenage is fantastic',\n",
       " 'Being teenage is wonderful',\n",
       " 'Being teenage is lovely',\n",
       " 'Being teenage is excellent',\n",
       " 'Being teenage is incredible']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1000:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788fc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.1)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e7360df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# local_files_only=True (Parameter for .from_pretrained to specify taking the tokenizer from local files / .cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0f01c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "18dbd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "432bfc54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file config.json from cache at /Users/jayjoshi/.cache/huggingface/hub/models--Hate-speech-CNERG--dehatebert-mono-english/snapshots/25d0e4d9122d2a5c283e07405a325e3dfd4a73b3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"Hate-speech-CNERG/dehatebert-mono-english\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.3,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NON_HATE\",\n",
      "    \"1\": \"HATE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"HATE\": 1,\n",
      "    \"NON_HATE\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1be718d78c846369127630f8f4892d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /Users/jayjoshi/.cache/huggingface/hub/models--Hate-speech-CNERG--dehatebert-mono-english/snapshots/25d0e4d9122d2a5c283e07405a325e3dfd4a73b3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at Hate-speech-CNERG/dehatebert-mono-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/cleanData',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs/cleandata/',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    # use_mps_device=True\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cb7652a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "674b9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "aafea662",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4cb87dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6501547987616099}\n",
      "{'accuracy': 0.6501547987616099, 'f1': 0.531464605018788}\n",
      "{'precision': 0.8074676816064046}\n",
      "{'recall': 0.3960794412077124}\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "values = list(score_dict.values())\n",
    "for i in values: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7942c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels predicted 1:  15239\n",
      "Labels predicted 0:  46777\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels predicted 1: \",(preds==1).sum())\n",
    "print(\"Labels predicted 0: \",(preds==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d74b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
