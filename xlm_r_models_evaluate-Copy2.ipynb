{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662c9d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: transformers in ./.env/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.env/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.env/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./.env/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: torch in ./.env/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in ./.env/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in ./.env/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.env/lib/python3.10/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: numpy in ./.env/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.env/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: scikit-learn in ./.env/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.env/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: sentencepiece in ./.env/lib/python3.10/site-packages (0.1.97)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: sacremoses in ./.env/lib/python3.10/site-packages (0.0.53)\n",
      "Requirement already satisfied: regex in ./.env/lib/python3.10/site-packages (from sacremoses) (2022.10.31)\n",
      "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in ./.env/lib/python3.10/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./.env/lib/python3.10/site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.10/site-packages (from sacremoses) (4.64.1)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: evaluate in ./.env/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: multiprocess in ./.env/lib/python3.10/site-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (from evaluate) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.6.1)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: dill in ./.env/lib/python3.10/site-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.env/lib/python3.10/site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: responses<0.19 in ./.env/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.env/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: xxhash in ./.env/lib/python3.10/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: aiohttp in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.0)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: datasets in ./.env/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (from datasets) (1.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in ./.env/lib/python3.10/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.env/lib/python3.10/site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: multiprocess in ./.env/lib/python3.10/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./.env/lib/python3.10/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in ./.env/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: xxhash in ./.env/lib/python3.10/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.env/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: responses<0.19 in ./.env/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: dill<0.3.6 in ./.env/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas->datasets) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch torchvision\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install evaluate\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14c2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_hindi = \"Hate-speech-CNERG/hindi-abusive-MuRIL\"\n",
    "# checkpoint_spanish = \"Hate-speech-CNERG/dehatebert-mono-spanish\"\n",
    "# checkpoint_english = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "# checkpoint_xlm_r = \"FineTuned_Model/xlm_r/\"\n",
    "# checkpoint = \"Hate-speech-CNERG/hindi-abusive-MuRIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae700e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6a05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "metric_glue = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "metric_rocauc = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(labels):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "    score_dict[\"accuracy\"] = metric_accuracy.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"glue\"] = metric_glue.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"precision\"] = metric_precision.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"recall\"] = metric_recall.compute(predictions=preds, references=labels)\n",
    "#     score_dict[\"rocauc\"] = metric_rocauc.compute(predictions=preds, references=labels)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed2f94e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ethos (/Users/jayjoshi/.cache/huggingface/datasets/ethos/binary/1.0.0/898d3d005459ee3ff80dbeec2f169c6b7ea13de31a08458193e27dec3dd9ae38)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d434ac570f4a4f3b9859fe55c5057a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_names=['HI-EN-ES', 'HI-EN', 'HI-ES', 'EN-ES', 'HI', 'ES', 'EN', 'en1']\n",
    "# datasets = [\"My_Data/en_sentence_templates.txt\", \"My_Data/es-es_sentence_templates.txt\", \"My_Data/hi-Latn_sentence_templates.txt\"]\n",
    "# datasets = [\"test_data/hateval.csv\", \"test_data/R_36.csv\"]\n",
    "# datasets = [ \"test_data/R_63.csv\"]\n",
    "datasets = [datasets.load_dataset(\"ethos\", \"binary\")]\n",
    "results = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ece88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717ade44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ethos (/Users/jayjoshi/.cache/huggingface/datasets/ethos/binary/1.0.0/898d3d005459ee3ff80dbeec2f169c6b7ea13de31a08458193e27dec3dd9ae38)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c56bd39fe24571b846e1b64e98fd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"ethos\", \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c49953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"You should know women's sports are a joke\", 'label': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed0b6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "for i in dataset['train']:\n",
    "    texts.append(i['text'])\n",
    "    labels.append(int(i['label']))\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.1)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.1)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665f682d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d4556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/EN-ES//config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/EN-ES//\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/EN-ES//pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/EN-ES//.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 808\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"results/xlm_r/EN-ES//\"\n",
    "model = model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "training_args = TrainingArguments(\n",
    "            output_dir='./results/test_data/xyz',          # output directory\n",
    "            num_train_epochs=3,              # total number of training epochs\n",
    "            per_device_train_batch_size=16,  # batch size per device during training\n",
    "            per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "            warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,               # strength of weight decay\n",
    "            logging_dir='./logs/test_data/xyz' ,            # directory for storing logs\n",
    "            logging_steps=10,\n",
    "            # use_mps_device=True\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "args=training_args,                  # training arguments, defined above\n",
    "train_dataset=train_dataset,         # training dataset\n",
    "eval_dataset=val_dataset,            # evaluation dataset\n",
    "\n",
    "        )\n",
    "score_dict = {\"accuracy\": \"none\", \"glue\": \"none\", \"precision\": \"none\", \"recall\": \"none\"}\n",
    "\n",
    "x = []\n",
    "\n",
    "predictions = trainer.predict(train_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds_list = list(preds)\n",
    "compute_metrics(predictions.label_ids)\n",
    "values = list(score_dict.values())\n",
    "x.append(values[0][\"accuracy\"])\n",
    "x.append(values[1][\"f1\"])\n",
    "x.append(values[2][\"precision\"])\n",
    "x.append(values[3][\"recall\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5dd92d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6868811881188119, 0.56, 0.7092511013215859, 0.46264367816091956]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5df6de11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               train\n",
      "0  {'text': 'You should know women's sports are a...\n",
      "1  {'text': 'You look like Sloth with deeper Down...\n",
      "2  {'text': 'You look like Russian and speak like...\n",
      "3  {'text': 'Women deserve to be abused, I guess....\n",
      "4  {'text': 'Women are made for making babies and...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/xlm/.env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/xlm/.env/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/xlm/.env/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#         ds.rename(columns={\"phrase\" : \"Text\", \"toxicity\": \"Label\"}, inplace=True)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(ds\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m---> 14\u001b[0m         texts \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#         labels =[0 if i == \"nontoxic\" else 1 for i in  ds[\"Label\"].tolist()]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()]\n",
      "File \u001b[0;32m~/Desktop/xlm/.env/lib/python3.10/site-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3804\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3806\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/xlm/.env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    \n",
    "    checkpoint = \"results/xlm_r/\" + model_name \n",
    "    # local_files_only=True (Parameter for .from_pretrained to specify taking the tokenizer from local files / .cache)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        ds = pd.DataFrame(dataset)\n",
    "        ds.dropna(inplace=True)\n",
    "#         ds.rename(columns={\"phrase\" : \"Text\", \"toxicity\": \"Label\"}, inplace=True)\n",
    "\n",
    "        print(ds.head())\n",
    "        texts = ds[\"text\"].tolist()\n",
    "#         labels =[0 if i == \"nontoxic\" else 1 for i in  ds[\"Label\"].tolist()]\n",
    "        labels = [int(i) for i in ds[\"label\"].tolist()]\n",
    "\n",
    "        train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.1)\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.1)\n",
    "\n",
    "        \n",
    "        train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "        val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "        test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "        train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "        val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "        test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results/test_data/'+ model_name + \"_\" + dataset,          # output directory\n",
    "            num_train_epochs=3,              # total number of training epochs\n",
    "            per_device_train_batch_size=16,  # batch size per device during training\n",
    "            per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "            warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,               # strength of weight decay\n",
    "            logging_dir='./logs/test_data/' + model_name + \"_\" + dataset,            # directory for storing logs\n",
    "            logging_steps=10,\n",
    "            # use_mps_device=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset,         # training dataset\n",
    "            eval_dataset=val_dataset,            # evaluation dataset\n",
    "\n",
    "        )\n",
    "\n",
    "        score_dict = {\"accuracy\": \"none\", \"glue\": \"none\", \"precision\": \"none\", \"recall\": \"none\"}\n",
    "\n",
    "        predictions = trainer.predict(train_dataset)\n",
    "        preds = np.argmax(predictions.predictions, axis=-1)\n",
    "        preds_list = list(preds)\n",
    "        compute_metrics(predictions.label_ids)\n",
    "        \n",
    "        #Confusion Matrix\n",
    "        test_positives_count = 0\n",
    "        test_negatives_count = 0\n",
    "        pred_poisitves_count = (preds==1).sum()\n",
    "        pred_negatives_count = (preds==0).sum()\n",
    "        \n",
    "        true_positives_count = 0\n",
    "        true_negatives_count = 0\n",
    "        false_positives_count = 0\n",
    "        false_negatives_count = 0\n",
    "        for i in range(len(test_labels)):\n",
    "            if train_labels[i] == 1:\n",
    "                test_positives_count += 1\n",
    "                if preds_list[i] == 1:\n",
    "                    true_positives_count += 1\n",
    "                else:\n",
    "                    false_negatives_count += 1\n",
    "            else:\n",
    "                test_negatives_count += 1\n",
    "                if preds_list[i] == 0:\n",
    "                    true_negatives_count += 1\n",
    "                else:\n",
    "                    false_positives_count += 1\n",
    "                    \n",
    "        \n",
    "\n",
    "        values = list(score_dict.values())\n",
    "        x = []\n",
    "\n",
    "        x.append(model_name)\n",
    "        x.append(dataset)\n",
    "        x.append(values[0][\"accuracy\"])\n",
    "        x.append(values[1][\"f1\"])\n",
    "        x.append(values[2][\"precision\"])\n",
    "        x.append(values[3][\"recall\"])\n",
    "        x.append((preds==1).sum())\n",
    "        x.append((preds==0).sum())\n",
    "        x.append(test_positives_count)\n",
    "        x.append(test_negatives_count)\n",
    "        x.append(true_positives_count)\n",
    "        x.append(true_negatives_count)\n",
    "        x.append(false_positives_count)\n",
    "        x.append(false_negatives_count)\n",
    "        \n",
    "        results.append(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d666549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=[['HI-EN-ES', 'My_Data/en_sentence_templates.txt', 0.5464557533539731, 0.24940623915886106, 0.7294723696534499, 0.1504168410210191, 6406, 55610], ['HI-EN-ES', 'My_Data/es-es_sentence_templates.txt', 0.6401228008029284, 0.5099212091976202, 0.7988714228133817, 0.37447451702801, 19848, 64842], ['HI-EN-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4793261868300153, 0.08832807570977919, 0.43209876543209874, 0.049191848208011243, 1296, 20906], ['HI-EN', 'My_Data/en_sentence_templates.txt', 0.541827915376677, 0.22594529802767788, 0.7351533416060982, 0.13348569221360285, 5641, 56375], ['HI-EN', 'My_Data/es-es_sentence_templates.txt', 0.6017357421183138, 0.3734745054332683, 0.874706342991386, 0.23742383449057675, 11493, 73197], ['HI-EN', 'My_Data/hi-Latn_sentence_templates.txt', 0.4936041798036213, 0.0807783500940234, 0.5832349468713105, 0.043394237526352776, 847, 21355], ['HI-ES', 'My_Data/en_sentence_templates.txt', 0.6197432920536635, 0.5050996852046169, 0.7256829283000663, 0.38735635883735153, 16583, 45433], ['HI-ES', 'My_Data/es-es_sentence_templates.txt', 0.6655331207934821, 0.5779042737080526, 0.7829685859646289, 0.4579613622408011, 24766, 59924], ['HI-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4877488514548239, 0.04740765558254461, 0.5099099099099099, 0.024859451862262825, 555, 21647], ['EN-ES', 'My_Data/en_sentence_templates.txt', 0.5744646542827657, 0.35047994093034707, 0.7445362333995608, 0.2291820903209193, 9563, 52453], ['EN-ES', 'My_Data/es-es_sentence_templates.txt', 0.6137914747904121, 0.432290762661853, 0.8154138292299633, 0.29410514382882247, 15272, 69418], ['EN-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4877038104675254, 0.010268012530455969, 0.5462962962962963, 0.005182712579058327, 108, 22094], ['HI', 'My_Data/en_sentence_templates.txt', 0.7152186532507739, 0.7054683721628339, 0.7319698228128461, 0.6808188753339557, 28896, 33120], ['HI', 'My_Data/es-es_sentence_templates.txt', 0.6705041917581769, 0.6989719414448916, 0.6433465059475346, 0.7651268244296443, 50357, 34333], ['HI', 'My_Data/hi-Latn_sentence_templates.txt', 0.49635168002882624, 0.12229199372056518, 0.5744837758112095, 0.06842937456078707, 1356, 20846], ['ES', 'My_Data/en_sentence_templates.txt', 0.5861390608875129, 0.39866922824609907, 0.7325010761945759, 0.27385972253516594, 11615, 50401], ['ES', 'My_Data/es-es_sentence_templates.txt', 0.6323414806942969, 0.4815943259577444, 0.8161503301168106, 0.34157574039960326, 17721, 66969], ['ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4936942617782182, 0.03667837860999229, 0.7508771929824561, 0.018798313422347154, 285, 21917], ['EN', 'My_Data/en_sentence_templates.txt', 0.5406830495356038, 0.2487538571089485, 0.6884671532846716, 0.15180094634177746, 6850, 55166], ['EN', 'My_Data/es-es_sentence_templates.txt', 0.6057503837525091, 0.41159573530707555, 0.8108033048670416, 0.27580180435501395, 14403, 70287], ['EN', 'My_Data/hi-Latn_sentence_templates.txt', 0.49193766327357896, 0.02371473082915008, 0.8058823529411765, 0.012034434293745608, 170, 22032], ['en1', 'My_Data/en_sentence_templates.txt', 0.5459881320949432, 0.26726695466611144, 0.6977850251392852, 0.16528792609521356, 7359, 54657], ['en1', 'My_Data/es-es_sentence_templates.txt', 0.6016767032707522, 0.40746855898264595, 0.7949965729952022, 0.27393604458929666, 14590, 70100], ['en1', 'My_Data/hi-Latn_sentence_templates.txt', 0.4887397531753896, 0.011839470706015497, 0.6601941747572816, 0.0059732958538299364, 103, 22099]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ad9d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features.append(\"Model\")\n",
    "features.append(\"Dataset\")\n",
    "features.append(\"Accuracy\")\n",
    "features.append(\"F1\")\n",
    "features.append(\"Precision\")\n",
    "features.append(\"Recall\")\n",
    "features.append(\"Preds == 1\")\n",
    "features.append(\"Preds == 0\")\n",
    "features.append(\"Test_Pos_Count\")\n",
    "features.append(\"Test_Neg_Count\")\n",
    "features.append(\"True_Pos_Count\")\n",
    "features.append(\"True_Neg_Count\")\n",
    "features.append(\"False_Pos_Count\")\n",
    "features.append(\"False_Neg_Count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "374f90fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model', 'Dataset', 'Accuracy', 'F1', 'Precision', 'Recall', 'Preds == 1', 'Preds == 0', 'Test_Pos_Count', 'Test_Neg_Count', 'True_Pos_Count', 'True_Neg_Count', 'False_Pos_Count', 'False_Neg_Count']\n",
      "['HI-EN-ES', 'test_data/R_63.csv', 0.6237623762376238, 0.62, 0.8888888888888888, 0.4760076775431862, 279, 529, 67, 33, 33, 32, 1, 34]\n",
      "['HI-EN', 'test_data/R_63.csv', 0.6175742574257426, 0.6171003717472119, 0.8706293706293706, 0.4779270633397313, 286, 522, 67, 33, 34, 28, 5, 33]\n",
      "['HI-ES', 'test_data/R_63.csv', 0.6732673267326733, 0.7130434782608696, 0.8220551378446115, 0.6295585412667947, 399, 409, 67, 33, 40, 28, 5, 27]\n",
      "['EN-ES', 'test_data/R_63.csv', 0.5829207920792079, 0.5548216644649934, 0.8898305084745762, 0.40307101727447214, 236, 572, 67, 33, 23, 29, 4, 44]\n",
      "['HI', 'test_data/R_63.csv', 0.6844059405940595, 0.7312961011591147, 0.8107476635514018, 0.6660268714011516, 428, 380, 67, 33, 43, 25, 8, 24]\n",
      "['ES', 'test_data/R_63.csv', 0.6014851485148515, 0.5985037406483791, 0.8540925266903915, 0.46065259117082535, 281, 527, 67, 33, 25, 27, 6, 42]\n",
      "['EN', 'test_data/R_63.csv', 0.5866336633663366, 0.5616797900262467, 0.8879668049792531, 0.4107485604606526, 241, 567, 67, 33, 24, 30, 3, 43]\n",
      "['en1', 'test_data/R_63.csv', 0.5878712871287128, 0.5680933852140078, 0.876, 0.42034548944337813, 250, 558, 67, 33, 23, 29, 4, 44]\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "for i in results: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb5ceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c477b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_results = list(zip(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b91f4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"results5.csv\",index=False, header=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd7023",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb7ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
