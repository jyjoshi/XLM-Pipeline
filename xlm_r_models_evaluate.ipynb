{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "662c9d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: transformers in ./.env/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.env/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.env/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./.env/lib/python3.10/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.env/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: torch in ./.env/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in ./.env/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in ./.env/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.env/lib/python3.10/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: numpy in ./.env/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.env/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: scikit-learn in ./.env/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.env/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.env/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: sentencepiece in ./.env/lib/python3.10/site-packages (0.1.97)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: sacremoses in ./.env/lib/python3.10/site-packages (0.0.53)\n",
      "Requirement already satisfied: regex in ./.env/lib/python3.10/site-packages (from sacremoses) (2022.10.31)\n",
      "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in ./.env/lib/python3.10/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./.env/lib/python3.10/site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in ./.env/lib/python3.10/site-packages (from sacremoses) (4.64.1)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/jayjoshi/.zshenv:3: unmatched \"\n",
      "Requirement already satisfied: evaluate in ./.env/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: dill in ./.env/lib/python3.10/site-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (from evaluate) (1.5.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in ./.env/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.env/lib/python3.10/site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: multiprocess in ./.env/lib/python3.10/site-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: xxhash in ./.env/lib/python3.10/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.env/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.env/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch torchvision\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b14c2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_hindi = \"Hate-speech-CNERG/hindi-abusive-MuRIL\"\n",
    "# checkpoint_spanish = \"Hate-speech-CNERG/dehatebert-mono-spanish\"\n",
    "# checkpoint_english = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "# checkpoint_xlm_r = \"FineTuned_Model/xlm_r/\"\n",
    "# checkpoint = \"Hate-speech-CNERG/hindi-abusive-MuRIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ae700e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c6a05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "metric_glue = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "metric_rocauc = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(labels):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "    score_dict[\"accuracy\"] = metric_accuracy.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"glue\"] = metric_glue.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"precision\"] = metric_precision.compute(predictions=preds, references=labels)\n",
    "    score_dict[\"recall\"] = metric_recall.compute(predictions=preds, references=labels)\n",
    "#     score_dict[\"rocauc\"] = metric_rocauc.compute(predictions=preds, references=labels)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed2f94e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/jayjoshi/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /Users/jayjoshi/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /Users/jayjoshi/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/jayjoshi/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/42f548f32366559214515ec137cdd16002968bf6/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_names=['HI-EN-ES', 'HI-EN', 'HI-ES', 'EN-ES', 'HI', 'ES', 'EN', 'en1']\n",
    "datasets = [\"My_Data/en_sentence_templates.txt\", \"My_Data/es-es_sentence_templates.txt\", \"My_Data/hi-Latn_sentence_templates.txt\"]\n",
    "results = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5df6de11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/HI-EN-ES/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/HI-EN-ES\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/HI-EN-ES/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/HI-EN-ES.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/HI-EN/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/HI-EN\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/HI-EN/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/HI-EN.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/HI-ES/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/HI-ES\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/HI-ES/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/HI-ES.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/EN-ES/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/EN-ES\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/EN-ES/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/EN-ES.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/HI/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/HI\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/HI/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/HI.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/ES/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/ES\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/ES/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/ES.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/EN/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/EN\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/EN/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/EN.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/xlm_r/en1/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"results/xlm_r/en1\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file results/xlm_r/en1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at results/xlm_r/en1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 62016\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 84690\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 22202\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    \n",
    "    checkpoint = \"results/xlm_r/\" + model_name\n",
    "    # local_files_only=True (Parameter for .from_pretrained to specify taking the tokenizer from local files / .cache)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        ds = pd.read_csv(dataset)\n",
    "        ds.dropna(inplace=True)\n",
    "        ds.rename(columns={\"phrase\" : \"Text\", \"toxicity\": \"Label\"}, inplace=True)\n",
    "\n",
    "\n",
    "        texts = ds[\"Text\"].tolist()\n",
    "        labels =[0 if i == \"nontoxic\" else 1 for i in  ds[\"Label\"].tolist()]\n",
    "\n",
    "        train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=.1)\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.1)\n",
    "\n",
    "        \n",
    "        train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "        val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "        test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "        train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "        val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "        test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results/ConversationAI_'+ model_name + \"_\" + dataset,          # output directory\n",
    "            num_train_epochs=3,              # total number of training epochs\n",
    "            per_device_train_batch_size=16,  # batch size per device during training\n",
    "            per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "            warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,               # strength of weight decay\n",
    "            logging_dir='./logs/ConversationAI_/' + model_name + \"_\" + dataset,            # directory for storing logs\n",
    "            logging_steps=10,\n",
    "            # use_mps_device=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,                         # the instantiated  Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=train_dataset,         # training dataset\n",
    "            eval_dataset=val_dataset,            # evaluation dataset\n",
    "\n",
    "        )\n",
    "\n",
    "        score_dict = {\"accuracy\": \"none\", \"glue\": \"none\", \"precision\": \"none\", \"recall\": \"none\"}\n",
    "\n",
    "        predictions = trainer.predict(train_dataset)\n",
    "        preds = np.argmax(predictions.predictions, axis=-1)\n",
    "        compute_metrics(predictions.label_ids)\n",
    "\n",
    "        values = list(score_dict.values())\n",
    "        x = []\n",
    "\n",
    "        x.append(model_name)\n",
    "        x.append(dataset)\n",
    "        x.append(values[0][\"accuracy\"])\n",
    "        x.append(values[1][\"f1\"])\n",
    "        x.append(values[2][\"precision\"])\n",
    "        x.append(values[3][\"recall\"])\n",
    "        x.append((preds==1).sum())\n",
    "        x.append((preds==0).sum())\n",
    "        results.append(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d666549",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[['HI-EN-ES', 'My_Data/en_sentence_templates.txt', 0.5464557533539731, 0.24940623915886106, 0.7294723696534499, 0.1504168410210191, 6406, 55610], ['HI-EN-ES', 'My_Data/es-es_sentence_templates.txt', 0.6401228008029284, 0.5099212091976202, 0.7988714228133817, 0.37447451702801, 19848, 64842], ['HI-EN-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4793261868300153, 0.08832807570977919, 0.43209876543209874, 0.049191848208011243, 1296, 20906], ['HI-EN', 'My_Data/en_sentence_templates.txt', 0.541827915376677, 0.22594529802767788, 0.7351533416060982, 0.13348569221360285, 5641, 56375], ['HI-EN', 'My_Data/es-es_sentence_templates.txt', 0.6017357421183138, 0.3734745054332683, 0.874706342991386, 0.23742383449057675, 11493, 73197], ['HI-EN', 'My_Data/hi-Latn_sentence_templates.txt', 0.4936041798036213, 0.0807783500940234, 0.5832349468713105, 0.043394237526352776, 847, 21355], ['HI-ES', 'My_Data/en_sentence_templates.txt', 0.6197432920536635, 0.5050996852046169, 0.7256829283000663, 0.38735635883735153, 16583, 45433], ['HI-ES', 'My_Data/es-es_sentence_templates.txt', 0.6655331207934821, 0.5779042737080526, 0.7829685859646289, 0.4579613622408011, 24766, 59924], ['HI-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4877488514548239, 0.04740765558254461, 0.5099099099099099, 0.024859451862262825, 555, 21647], ['EN-ES', 'My_Data/en_sentence_templates.txt', 0.5744646542827657, 0.35047994093034707, 0.7445362333995608, 0.2291820903209193, 9563, 52453], ['EN-ES', 'My_Data/es-es_sentence_templates.txt', 0.6137914747904121, 0.432290762661853, 0.8154138292299633, 0.29410514382882247, 15272, 69418], ['EN-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4877038104675254, 0.010268012530455969, 0.5462962962962963, 0.005182712579058327, 108, 22094], ['HI', 'My_Data/en_sentence_templates.txt', 0.7152186532507739, 0.7054683721628339, 0.7319698228128461, 0.6808188753339557, 28896, 33120], ['HI', 'My_Data/es-es_sentence_templates.txt', 0.6705041917581769, 0.6989719414448916, 0.6433465059475346, 0.7651268244296443, 50357, 34333], ['HI', 'My_Data/hi-Latn_sentence_templates.txt', 0.49635168002882624, 0.12229199372056518, 0.5744837758112095, 0.06842937456078707, 1356, 20846], ['ES', 'My_Data/en_sentence_templates.txt', 0.5861390608875129, 0.39866922824609907, 0.7325010761945759, 0.27385972253516594, 11615, 50401], ['ES', 'My_Data/es-es_sentence_templates.txt', 0.6323414806942969, 0.4815943259577444, 0.8161503301168106, 0.34157574039960326, 17721, 66969], ['ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4936942617782182, 0.03667837860999229, 0.7508771929824561, 0.018798313422347154, 285, 21917], ['EN', 'My_Data/en_sentence_templates.txt', 0.5406830495356038, 0.2487538571089485, 0.6884671532846716, 0.15180094634177746, 6850, 55166], ['EN', 'My_Data/es-es_sentence_templates.txt', 0.6057503837525091, 0.41159573530707555, 0.8108033048670416, 0.27580180435501395, 14403, 70287], ['EN', 'My_Data/hi-Latn_sentence_templates.txt', 0.49193766327357896, 0.02371473082915008, 0.8058823529411765, 0.012034434293745608, 170, 22032], ['en1', 'My_Data/en_sentence_templates.txt', 0.5459881320949432, 0.26726695466611144, 0.6977850251392852, 0.16528792609521356, 7359, 54657], ['en1', 'My_Data/es-es_sentence_templates.txt', 0.6016767032707522, 0.40746855898264595, 0.7949965729952022, 0.27393604458929666, 14590, 70100], ['en1', 'My_Data/hi-Latn_sentence_templates.txt', 0.4887397531753896, 0.011839470706015497, 0.6601941747572816, 0.0059732958538299364, 103, 22099]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ad9d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features.append(\"Model\")\n",
    "features.append(\"Dataset\")\n",
    "features.append(\"Accuracy\")\n",
    "features.append(\"F1\")\n",
    "features.append(\"Precision\")\n",
    "features.append(\"Recall\")\n",
    "features.append(\"Preds == 1\")\n",
    "features.append(\"Preds == 0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "374f90fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model', 'Dataset', 'Accuracy', 'F1', 'Precision', 'Recall', 'Pred == 1', 'Pred == 0']\n",
      "['HI-EN-ES', 'My_Data/en_sentence_templates.txt', 0.5464557533539731, 0.24940623915886106, 0.7294723696534499, 0.1504168410210191, 6406, 55610]\n",
      "['HI-EN-ES', 'My_Data/es-es_sentence_templates.txt', 0.6401228008029284, 0.5099212091976202, 0.7988714228133817, 0.37447451702801, 19848, 64842]\n",
      "['HI-EN-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4793261868300153, 0.08832807570977919, 0.43209876543209874, 0.049191848208011243, 1296, 20906]\n",
      "['HI-EN', 'My_Data/en_sentence_templates.txt', 0.541827915376677, 0.22594529802767788, 0.7351533416060982, 0.13348569221360285, 5641, 56375]\n",
      "['HI-EN', 'My_Data/es-es_sentence_templates.txt', 0.6017357421183138, 0.3734745054332683, 0.874706342991386, 0.23742383449057675, 11493, 73197]\n",
      "['HI-EN', 'My_Data/hi-Latn_sentence_templates.txt', 0.4936041798036213, 0.0807783500940234, 0.5832349468713105, 0.043394237526352776, 847, 21355]\n",
      "['HI-ES', 'My_Data/en_sentence_templates.txt', 0.6197432920536635, 0.5050996852046169, 0.7256829283000663, 0.38735635883735153, 16583, 45433]\n",
      "['HI-ES', 'My_Data/es-es_sentence_templates.txt', 0.6655331207934821, 0.5779042737080526, 0.7829685859646289, 0.4579613622408011, 24766, 59924]\n",
      "['HI-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4877488514548239, 0.04740765558254461, 0.5099099099099099, 0.024859451862262825, 555, 21647]\n",
      "['EN-ES', 'My_Data/en_sentence_templates.txt', 0.5744646542827657, 0.35047994093034707, 0.7445362333995608, 0.2291820903209193, 9563, 52453]\n",
      "['EN-ES', 'My_Data/es-es_sentence_templates.txt', 0.6137914747904121, 0.432290762661853, 0.8154138292299633, 0.29410514382882247, 15272, 69418]\n",
      "['EN-ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4877038104675254, 0.010268012530455969, 0.5462962962962963, 0.005182712579058327, 108, 22094]\n",
      "['HI', 'My_Data/en_sentence_templates.txt', 0.7152186532507739, 0.7054683721628339, 0.7319698228128461, 0.6808188753339557, 28896, 33120]\n",
      "['HI', 'My_Data/es-es_sentence_templates.txt', 0.6705041917581769, 0.6989719414448916, 0.6433465059475346, 0.7651268244296443, 50357, 34333]\n",
      "['HI', 'My_Data/hi-Latn_sentence_templates.txt', 0.49635168002882624, 0.12229199372056518, 0.5744837758112095, 0.06842937456078707, 1356, 20846]\n",
      "['ES', 'My_Data/en_sentence_templates.txt', 0.5861390608875129, 0.39866922824609907, 0.7325010761945759, 0.27385972253516594, 11615, 50401]\n",
      "['ES', 'My_Data/es-es_sentence_templates.txt', 0.6323414806942969, 0.4815943259577444, 0.8161503301168106, 0.34157574039960326, 17721, 66969]\n",
      "['ES', 'My_Data/hi-Latn_sentence_templates.txt', 0.4936942617782182, 0.03667837860999229, 0.7508771929824561, 0.018798313422347154, 285, 21917]\n",
      "['EN', 'My_Data/en_sentence_templates.txt', 0.5406830495356038, 0.2487538571089485, 0.6884671532846716, 0.15180094634177746, 6850, 55166]\n",
      "['EN', 'My_Data/es-es_sentence_templates.txt', 0.6057503837525091, 0.41159573530707555, 0.8108033048670416, 0.27580180435501395, 14403, 70287]\n",
      "['EN', 'My_Data/hi-Latn_sentence_templates.txt', 0.49193766327357896, 0.02371473082915008, 0.8058823529411765, 0.012034434293745608, 170, 22032]\n",
      "['en1', 'My_Data/en_sentence_templates.txt', 0.5459881320949432, 0.26726695466611144, 0.6977850251392852, 0.16528792609521356, 7359, 54657]\n",
      "['en1', 'My_Data/es-es_sentence_templates.txt', 0.6016767032707522, 0.40746855898264595, 0.7949965729952022, 0.27393604458929666, 14590, 70100]\n",
      "['en1', 'My_Data/hi-Latn_sentence_templates.txt', 0.4887397531753896, 0.011839470706015497, 0.6601941747572816, 0.0059732958538299364, 103, 22099]\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "for i in results: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb5ceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c477b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['HI-EN-ES',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.5464557533539731,\n",
       "   0.24940623915886106,\n",
       "   0.7294723696534499,\n",
       "   0.1504168410210191,\n",
       "   6406,\n",
       "   55610],),\n",
       " (['HI-EN-ES',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6401228008029284,\n",
       "   0.5099212091976202,\n",
       "   0.7988714228133817,\n",
       "   0.37447451702801,\n",
       "   19848,\n",
       "   64842],),\n",
       " (['HI-EN-ES',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.4793261868300153,\n",
       "   0.08832807570977919,\n",
       "   0.43209876543209874,\n",
       "   0.049191848208011243,\n",
       "   1296,\n",
       "   20906],),\n",
       " (['HI-EN',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.541827915376677,\n",
       "   0.22594529802767788,\n",
       "   0.7351533416060982,\n",
       "   0.13348569221360285,\n",
       "   5641,\n",
       "   56375],),\n",
       " (['HI-EN',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6017357421183138,\n",
       "   0.3734745054332683,\n",
       "   0.874706342991386,\n",
       "   0.23742383449057675,\n",
       "   11493,\n",
       "   73197],),\n",
       " (['HI-EN',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.4936041798036213,\n",
       "   0.0807783500940234,\n",
       "   0.5832349468713105,\n",
       "   0.043394237526352776,\n",
       "   847,\n",
       "   21355],),\n",
       " (['HI-ES',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.6197432920536635,\n",
       "   0.5050996852046169,\n",
       "   0.7256829283000663,\n",
       "   0.38735635883735153,\n",
       "   16583,\n",
       "   45433],),\n",
       " (['HI-ES',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6655331207934821,\n",
       "   0.5779042737080526,\n",
       "   0.7829685859646289,\n",
       "   0.4579613622408011,\n",
       "   24766,\n",
       "   59924],),\n",
       " (['HI-ES',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.4877488514548239,\n",
       "   0.04740765558254461,\n",
       "   0.5099099099099099,\n",
       "   0.024859451862262825,\n",
       "   555,\n",
       "   21647],),\n",
       " (['EN-ES',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.5744646542827657,\n",
       "   0.35047994093034707,\n",
       "   0.7445362333995608,\n",
       "   0.2291820903209193,\n",
       "   9563,\n",
       "   52453],),\n",
       " (['EN-ES',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6137914747904121,\n",
       "   0.432290762661853,\n",
       "   0.8154138292299633,\n",
       "   0.29410514382882247,\n",
       "   15272,\n",
       "   69418],),\n",
       " (['EN-ES',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.4877038104675254,\n",
       "   0.010268012530455969,\n",
       "   0.5462962962962963,\n",
       "   0.005182712579058327,\n",
       "   108,\n",
       "   22094],),\n",
       " (['HI',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.7152186532507739,\n",
       "   0.7054683721628339,\n",
       "   0.7319698228128461,\n",
       "   0.6808188753339557,\n",
       "   28896,\n",
       "   33120],),\n",
       " (['HI',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6705041917581769,\n",
       "   0.6989719414448916,\n",
       "   0.6433465059475346,\n",
       "   0.7651268244296443,\n",
       "   50357,\n",
       "   34333],),\n",
       " (['HI',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.49635168002882624,\n",
       "   0.12229199372056518,\n",
       "   0.5744837758112095,\n",
       "   0.06842937456078707,\n",
       "   1356,\n",
       "   20846],),\n",
       " (['ES',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.5861390608875129,\n",
       "   0.39866922824609907,\n",
       "   0.7325010761945759,\n",
       "   0.27385972253516594,\n",
       "   11615,\n",
       "   50401],),\n",
       " (['ES',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6323414806942969,\n",
       "   0.4815943259577444,\n",
       "   0.8161503301168106,\n",
       "   0.34157574039960326,\n",
       "   17721,\n",
       "   66969],),\n",
       " (['ES',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.4936942617782182,\n",
       "   0.03667837860999229,\n",
       "   0.7508771929824561,\n",
       "   0.018798313422347154,\n",
       "   285,\n",
       "   21917],),\n",
       " (['EN',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.5406830495356038,\n",
       "   0.2487538571089485,\n",
       "   0.6884671532846716,\n",
       "   0.15180094634177746,\n",
       "   6850,\n",
       "   55166],),\n",
       " (['EN',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6057503837525091,\n",
       "   0.41159573530707555,\n",
       "   0.8108033048670416,\n",
       "   0.27580180435501395,\n",
       "   14403,\n",
       "   70287],),\n",
       " (['EN',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.49193766327357896,\n",
       "   0.02371473082915008,\n",
       "   0.8058823529411765,\n",
       "   0.012034434293745608,\n",
       "   170,\n",
       "   22032],),\n",
       " (['en1',\n",
       "   'My_Data/en_sentence_templates.txt',\n",
       "   0.5459881320949432,\n",
       "   0.26726695466611144,\n",
       "   0.6977850251392852,\n",
       "   0.16528792609521356,\n",
       "   7359,\n",
       "   54657],),\n",
       " (['en1',\n",
       "   'My_Data/es-es_sentence_templates.txt',\n",
       "   0.6016767032707522,\n",
       "   0.40746855898264595,\n",
       "   0.7949965729952022,\n",
       "   0.27393604458929666,\n",
       "   14590,\n",
       "   70100],),\n",
       " (['en1',\n",
       "   'My_Data/hi-Latn_sentence_templates.txt',\n",
       "   0.4887397531753896,\n",
       "   0.011839470706015497,\n",
       "   0.6601941747572816,\n",
       "   0.0059732958538299364,\n",
       "   103,\n",
       "   22099],)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_results = list(zip(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b91f4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"results.csv\",index=False, header=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd7023",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb7ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
